{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "music_similarity",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1pmtg_WhjiWIH3TNefBUIivuVjsLpQ8r8",
      "authorship_tag": "ABX9TyOy0blTFqdoGqMx24LpCfhn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minguezalba/MusiCNN-embeddings/blob/main/music_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozkWIRVwYo8h"
      },
      "source": [
        "# Music Similarity\n",
        "---\n",
        "Author: Alba Mínguez Sánchez\n",
        "\n",
        "March 2021\n",
        "\n",
        "---\n",
        "\n",
        "In this notebook, we will evaluate music similarity based on previously extracted songs embeddings. Each song embedding in the dataset has a single ground-truth genre label associated. We will assume these genres as a proxy for evaluating similarity, that is, considering two songs are similar when they have the same genre label. \n",
        "\n",
        "Evaluation metrics such as average precision@10, MAP@10, and MAP@50 will be computed across the entire dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0_tRhNScEjG"
      },
      "source": [
        "**Packages and dependencies**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdVsh6Vistxn"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import plotly.graph_objects as go\r\n",
        "import plotly.express as px\r\n",
        "\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CldxPD3rNXkR"
      },
      "source": [
        "## 1. Loading embeddings dataset from npy files\r\n",
        "\r\n",
        "This step requires to clone MusiCNN-embeddings repository to download the necessary files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFdRewJycANX"
      },
      "source": [
        "!git clone https://github.com/minguezalba/MusiCNN-embeddings.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvlDS1V3JmCj"
      },
      "source": [
        "with open('MusiCNN-embeddings/emb_dataset/embeddings.npy', 'rb') as f:\r\n",
        "    embeddings = np.load(f)\r\n",
        "with open('MusiCNN-embeddings/emb_dataset/labels.npy', 'rb') as f:\r\n",
        "    labels = np.load(f)\r\n",
        "with open('MusiCNN-embeddings/emb_dataset/labels_decoded.npy', 'rb') as f:\r\n",
        "    labels_decoded = np.load(f)\r\n",
        "with open('MusiCNN-embeddings/emb_dataset/track_ids.npy', 'rb') as f:\r\n",
        "    track_ids = np.load(f)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwYFyyzrLvQ-"
      },
      "source": [
        "Check data dimensions and types"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JknEzzvdOtN0"
      },
      "source": [
        "print('embeddings: ', embeddings.shape, type(embeddings))\r\n",
        "print('labels: ', labels.shape, type(labels))\r\n",
        "print('labels_decoded: ', labels_decoded.shape, type(labels_decoded))\r\n",
        "print('track_ids: ', track_ids.shape, type(track_ids))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfoZ1EiIMLdS"
      },
      "source": [
        "# 2. Computing similarity matrix\r\n",
        "\r\n",
        "In this step, we will compute similarity matrix between every song embedding with each other using cosine similarity distance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORPRkKYiMKna"
      },
      "source": [
        "similarities = cosine_similarity(embeddings)  # X as n-samples, n-features\r\n",
        "\r\n",
        "print(similarities.shape)\r\n",
        "print(f'Min value: {similarities.min()}')\r\n",
        "print(f'Max value: {similarities.max()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvAq2IR0MKre"
      },
      "source": [
        "# 3. Understanding similarity matrix\r\n",
        "\r\n",
        "We will visually represent similarity matrix in order to understand it and get conclusions from it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_HHh9s-MKuE"
      },
      "source": [
        "fig = px.imshow(similarities, color_continuous_scale='RdYlGn')\r\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGjxOtadfYff"
      },
      "source": [
        "As we have songs sorted by genre in blocks of 100 songs, we can visually check darker-green (more similar) areas for 100 to 100 songs corresponding to the similarity between the 100 songs within the same genre."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6aiFk3DMKwb"
      },
      "source": [
        "# 4. Compute similar songs matrix\r\n",
        "\r\n",
        "We want to evaluate how good is our system, which will *recommend* similar songs to a query song. We will use cosine similarity to sort the songs, and then we will evaluate whether our sorted recommended songs are truly relevant or not, checking their original genre (if it matches or not with the target song)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjr5xu7sMKzZ"
      },
      "source": [
        "We will start sorting our similarities matrix and saving the corresponding sorted indexes of the songs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aRZlF7mMK1S"
      },
      "source": [
        "sorted_indexes = np.argsort(similarities, axis=1)\r\n",
        "sorted_indexes = np.fliplr(sorted_indexes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRBbmAhXh8u9"
      },
      "source": [
        "We will compute what we call *similar songs matrix*, where each row would correspond to the query song, and the columns would be the recommended similar songs, sorted by similarity.\r\n",
        "\r\n",
        "Each `ij` item in the matrix contains 1 if song i is similar (aka belongs to the same genre) to song j, or 0 if it is not.\r\n",
        "\r\n",
        "Remember we will build this *similar songs matrix* from previous one, already sorted by similarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTD1Aymoi0xE"
      },
      "source": [
        "similar_songs = np.zeros_like((sorted_indexes))\r\n",
        "\r\n",
        "for i in range(sorted_indexes.shape[0]):\r\n",
        "  genre_i = labels[i]  # Label/genre of the queried song\r\n",
        "  sorted_indexes_i = sorted_indexes[i, :]  # Sorted similar songs for queried song\r\n",
        "\r\n",
        "  # Assign 1 if genres between songs match, 0 if not.\r\n",
        "  similar_songs_i = np.array([1 if genre_i == labels[j] else 0 for j in sorted_indexes_i])\r\n",
        "  similar_songs[i, :] = similar_songs_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ2__wP-kxvJ"
      },
      "source": [
        "# 5. Define evaluation metrics: AP@N and MAP@N\r\n",
        "\r\n",
        "We will manually implement AP@N and MAP@N evaluation metrics from the following formulas definitions, adapted to our task:\r\n",
        "\r\n",
        "**Precision@k**\r\n",
        "\r\n",
        "$$ P = \\frac{\\text{# similar items in the k recommended songs}}{\\text{k recommended songs}}$$\r\n",
        "\r\n",
        "\r\n",
        "**AveragePrecision@N**\r\n",
        "\r\n",
        "$$ AP@N = \\frac{1}{min(N, \\text{# similar items in the fullspace})}\\sum_{k=1}^{N}P(k)\\cdot rel(k)$$\r\n",
        "\r\n",
        "**MeanAveragePrecision@N**\r\n",
        "\r\n",
        "$$ mAP@N = \\frac{1}{\\text{N songs in the dataset}}\\sum_{i=1}^{N}AP@N_i$$\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plU0x_mx0b0y"
      },
      "source": [
        "def precision_k(similar_items, k):\r\n",
        "  \"\"\"\r\n",
        "  Compute precision at top-k (P@k) elements as the ratio between number of similar songs and number of songs evaluated in the top (k).\r\n",
        "  \r\n",
        "  Args:\r\n",
        "      similar_items (1d nparray): Vector where 1 represents truly similar and 0 not similar.\r\n",
        "      k (int): Number of top items to compute P.\r\n",
        "\r\n",
        "  Returns:\r\n",
        "      float: The precision (P) value at top k items.\r\n",
        "  \"\"\"\r\n",
        "  similar_items_k = similar_items[:k]\r\n",
        "  P = sum(similar_items_k) / k\r\n",
        "  return P"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBkBwnUACqLN"
      },
      "source": [
        "def average_precision_N(similar_items, N):\r\n",
        "  \"\"\"\r\n",
        "  Compute average precision (AP) at top-N elements.\r\n",
        "  \r\n",
        "  Args:\r\n",
        "      similar_items (1d nparray): Vector where 1 represents truly similar and 0 not similar.\r\n",
        "      N (int): Number of sorted items to evaluate AP.\r\n",
        "\r\n",
        "  Returns:\r\n",
        "      float: The AP value evaluating N items.\r\n",
        "  \"\"\"\r\n",
        "  m = np.min([np.sum(similar_items), N])  # min(number of relevant/similar items (1s) in the full space of items or N)\r\n",
        "  sum_vector = []\r\n",
        "  \r\n",
        "  for k in range(1, N+1):\r\n",
        "    sum_vector.append(precision_k(similar_items, k) * similar_items[k-1])\r\n",
        "\r\n",
        "  AP = (1/m)*sum(sum_vector)\r\n",
        "  return AP \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVBJUBnqpVR5"
      },
      "source": [
        "# 6. Evaluate music similarity\r\n",
        "\r\n",
        "We will apply the previous evaluation metrics for a set of N-top songs range. We will evaluate different levels: song, genre and whole dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3NY85yZsdg9"
      },
      "source": [
        "### 6.1. Average precision @ N for each song"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejmJo0M5ppg8"
      },
      "source": [
        "N_range = [10,25,50,75,100]\r\n",
        "AP_songs = []\r\n",
        "\r\n",
        "for N in N_range:\r\n",
        "  AP_by_N = []\r\n",
        "  for i in range(similar_songs.shape[0]):\r\n",
        "    AP_by_N.append(average_precision_N(similar_songs[i, :], N))\r\n",
        "  AP_songs.append(AP_by_N)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmfPDPeAu2q_"
      },
      "source": [
        "df = pd.DataFrame(list(zip([id for id in track_ids], [y for y in labels_decoded], *AP_songs)), \r\n",
        "                  columns =['song_id', 'genre']+[f'@{N}' for N in N_range])\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYd6xf13sn7L"
      },
      "source": [
        "from plotly.subplots import make_subplots\r\n",
        "\r\n",
        "fig = make_subplots(rows=2, cols=3, subplot_titles=(\"AP@10\", \"AP@25\", \"AP@50\", \"AP@75\", \"AP@100\"))\r\n",
        "\r\n",
        "fig.add_trace(go.Histogram(x=AP_songs[0]), row=1, col=1)\r\n",
        "fig.add_trace(go.Histogram(x=AP_songs[1]), row=1, col=2)\r\n",
        "fig.add_trace(go.Histogram(x=AP_songs[2]), row=1, col=3)\r\n",
        "fig.add_trace(go.Histogram(x=AP_songs[3]), row=2, col=1)\r\n",
        "fig.add_trace(go.Histogram(x=AP_songs[4]), row=2, col=2)\r\n",
        "\r\n",
        "fig.update_layout(height=500, \r\n",
        "                  width=1000, \r\n",
        "                  showlegend=False,\r\n",
        "                  title_text=\"AP@N histograms\")\r\n",
        "\r\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_fCbP91uQeI"
      },
      "source": [
        "We can observe the more recommendations we ask for (greater N), the less precise is our model when recommending truly similar songs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC_oPXveurF4"
      },
      "source": [
        "### 6.2. Mean Average precision @ N for each genre\r\n",
        "\r\n",
        "Now, let's compute mAP@N for each genre and for the whole dataset.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K8GiIo3FmSq"
      },
      "source": [
        "mAP_by_genre = df.groupby('genre').mean()\r\n",
        "mAP_by_genre"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-TDYlPFrmqp"
      },
      "source": [
        "mAP_dataset = df.mean()\r\n",
        "mAP_dataset.to_frame().rename(columns={0: 'mAP'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqqqtGdpHsA2"
      },
      "source": [
        "fig = go.Figure()\r\n",
        "for genre in df.genre.unique():\r\n",
        "  fig.add_trace(go.Scatter(y=mAP_by_genre.loc[genre], x=N_range, mode='lines+markers', name=genre))\r\n",
        "fig.add_trace(go.Scatter(y=mAP_dataset, x=N_range, mode='lines+markers', name='mean', line=dict(width=4, dash='dash')))\r\n",
        "\r\n",
        "fig.update_layout(title='MAP@N by genre', xaxis_title='N', yaxis_title='MAP@N')\r\n",
        "fig.show()\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dum34ju4r0Xp"
      },
      "source": [
        "From the figure above we can get several conclusions:\r\n",
        "*  Rock is the worst identified genre when looking for similar songs.\r\n",
        "*  The number of recommendations N does not affect to all genres in the same way. There are some genres such as classical, hip-hop, metal or disco, where a greater N does not affect much their precision. However, genres such as blues, country or pop, which are really affected by N. This means the system does not recognize so clearly the similar songs for those genred among the whole dataset, based on the embeddings representation and cosine similarity."
      ]
    }
  ]
}
