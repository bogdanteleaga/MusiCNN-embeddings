{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset_generation",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1hC9VEp5N5HQJPCFnDT9A8w0q4F0AfTd4",
      "authorship_tag": "ABX9TyM+NRv2L58NUq2YrLXE9GsN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minguezalba/MusiCNN-embeddings/blob/main/dataset_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozkWIRVwYo8h"
      },
      "source": [
        "# Dataset Generation\n",
        "---\n",
        "Author: Alba Mínguez Sánchez\n",
        "\n",
        "March 2021\n",
        "\n",
        "---\n",
        "\n",
        "In this notebook, we will do all the necessary tasks to build a songs-embedding dataset from the GTZAN dataset in order to use it in several tasks: evaluating songs similarity and genre classification. \n",
        "\n",
        "The steps are the following:\n",
        "\n",
        "1.   Download GTZAN dataset with the help of [mirdata](https://mirdata.readthedocs.io/en/latest/) library.\n",
        "2.   Download [Essentia’s MSD-MusiCNN TensorFlow model](https://essentia.upf.edu/models/classifiers/genre_tzanetakis/) for the specific task of genre classification.\n",
        "3.   Embeddings extraction: Extract songs-temporal embeddings using MusiCNN model and process them to get mean-embeddings.\n",
        "5.   Labels processing: Encode genre labels to numeric values.\n",
        "6.   Save data to npy files.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUY7l1FZ9Qhq"
      },
      "source": [
        "**Packages and dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtGu4wpa9QEW"
      },
      "source": [
        "!pip install mirdata\r\n",
        "!pip install essentia-tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dXSn2kk93_2"
      },
      "source": [
        "import essentia.standard as es\r\n",
        "import mirdata\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import json\r\n",
        "\r\n",
        "from collections import Counter\r\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2baS32QCiLJ2"
      },
      "source": [
        "# 1. Download GTZAN dataset\r\n",
        "\r\n",
        "Download GTZAN dataset using mirdata library and validate the download."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uviIAP188RS4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23971c5d-bbbb-4345-e9c1-69f10cc58b76"
      },
      "source": [
        "DATASET_NAME = 'gtzan_genre'\r\n",
        "\r\n",
        "dataset = mirdata.initialize(DATASET_NAME, data_home='/content/mydata/')\r\n",
        "dataset.download()  # download the dataset\r\n",
        "dataset.validate()  # validate that all the expected files are there"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: Downloading ['all'] to /content/mydata/\n",
            "INFO: [all] downloading genres.tar.gz\n",
            "1.14GB [12:11, 1.68MB/s]                            \n",
            "100%|██████████| 1000/1000 [00:03<00:00, 310.51it/s]\n",
            "INFO: Success: the dataset is complete and all files are valid.\n",
            "INFO: --------------------\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'tracks': {}}, {'tracks': {}})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkYdOcg1JF-x",
        "outputId": "82fc5571-24e1-4322-aa30-812c0c8d887b"
      },
      "source": [
        "%cd mydata"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mydata\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0sTQn2qBT5D"
      },
      "source": [
        "Load all tracks from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx0N1y3PBWfg"
      },
      "source": [
        "tracks = dataset.load_tracks()  # Return dict like {track_id: Track() object}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9_XWtGTBDpw"
      },
      "source": [
        "Check data attributes and genres distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHibmkhSAvjm",
        "outputId": "b80dfe37-9311-4b6d-ddbb-07df2eedfaa2"
      },
      "source": [
        "example_track = dataset.choice_track()  # choose a random example track\r\n",
        "print(example_track)  # see the available data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Track(\n",
            "  audio_path=\"/content/mydata/gtzan_genre/genres/hiphop/hiphop.00049.wav\",\n",
            "  genre=\"hip-hop\",\n",
            "  track_id=\"hiphop.00049\",\n",
            "  audio: The track's audio\n",
            "\n",
            "        Returns,\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DmaHzcFBp3i",
        "outputId": "68e1b4d7-67a3-4ec8-8fe3-6fa4fb40500c"
      },
      "source": [
        "Counter([x.genre for x in tracks.values()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'blues': 100,\n",
              "         'classical': 100,\n",
              "         'country': 100,\n",
              "         'disco': 100,\n",
              "         'hip-hop': 100,\n",
              "         'jazz': 100,\n",
              "         'metal': 100,\n",
              "         'pop': 100,\n",
              "         'reggae': 100,\n",
              "         'rock': 100})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYsKmnl3BwHw"
      },
      "source": [
        "We can observe data is totally balanced between the 10 differente classes/genres."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stTELaEf8Sqv"
      },
      "source": [
        "# 2. Download Essentia’s MSD-MusiCNN TensorFlow model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW0EtPXeCHDc"
      },
      "source": [
        "Download the necessary files to describe the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqXmihfuAk8F",
        "outputId": "ed8b8baa-2cfc-4de3-f5c5-f486a3374d52"
      },
      "source": [
        "!curl -SLO https://essentia.upf.edu/models/classifiers/genre_tzanetakis/genre_tzanetakis-musicnn-msd-1.json\r\n",
        "!curl -SLO https://essentia.upf.edu/models/classifiers/genre_tzanetakis/genre_tzanetakis-musicnn-msd-1.pb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  2092  100  2092    0     0   2716      0 --:--:-- --:--:-- --:--:--  2713\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 3166k  100 3166k    0     0  1121k      0  0:00:02  0:00:02 --:--:-- 1120k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8HRQhfSCLps"
      },
      "source": [
        "Explore model metadata, inputs and outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04Cc_sBZ8Wp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55fd0f5a-b1e1-48ba-d394-84d966308564"
      },
      "source": [
        "MODEL_NAME = 'genre_tzanetakis-musicnn-msd-1'\r\n",
        "MODEL_JSON = f'{MODEL_NAME}.json'\r\n",
        "MODEL_PB = f'{MODEL_NAME}.pb'\r\n",
        "\r\n",
        "musicnn_metadata = json.load(open(MODEL_JSON, 'r'))\r\n",
        "for k, v in musicnn_metadata.items():\r\n",
        "    print('{}: {}'.format(k , v))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name: genre GTZAN\n",
            "type: multi-class classifier\n",
            "link: https://essentia.upf.edu/models/classifiers/genre_tzanetakis/genre_tzanetakis-musicnn-msd-1.pb\n",
            "version: 1\n",
            "description: classification of music by genre\n",
            "author: Pablo Alonso\n",
            "email: pablo.alonso@upf.edu\n",
            "release_date: 2020-03-31\n",
            "framework: tensorflow\n",
            "framework_version: 1.15.0\n",
            "classes: ['blu', 'cla', 'cou', 'dis', 'hip', 'jaz', 'met', 'pop', 'reg', 'roc']\n",
            "model_types: ['frozen_model']\n",
            "dataset: {'name': 'the GTZAN Genre Collection', 'citation': '@article{tzanetakis2002musical,\\n  title={Musical genre classification of audio signals},\\n  author={Tzanetakis, George and Cook, Perry},\\n  journal={IEEE Transactions on speech and audio processing},\\n  volume={10},\\n  number={5},\\n  pages={293--302},\\n  year={2002},\\n  publisher={IEEE}\\n}', 'size': '1000 track excerpts, 100 per genre', 'metrics': {'5-fold_cross_validation_normalized_accuracy': 0.83}}\n",
            "schema: {'inputs': [{'name': 'model/Placeholder', 'type': 'float', 'shape': [187, 96]}], 'outputs': [{'name': 'model/Sigmoid', 'type': 'float', 'shape': [1, 10], 'op': 'Sigmoid'}, {'name': 'model/dense_2/BiasAdd', 'type': 'float', 'shape': [1, 10], 'op': 'fully connected', 'description': 'logits'}, {'name': 'model/dense_1/BiasAdd', 'type': 'float', 'shape': [1, 100], 'op': 'fully connected', 'description': 'penultimate layer'}, {'name': 'model/dense/BiasAdd', 'type': 'float', 'shape': [1, 200], 'op': 'fully connected', 'description': 'embeddings'}]}\n",
            "citation: @article{alonso2020tensorflow,\n",
            "title={TensorFlow Audio Models in Essentia},\n",
            "author={Alonso-Jim{\\'e}nez, Pablo and Bogdanov, Dmitry and Pons, Jordi and Serra, Xavier},\n",
            "journal={ICASSP 2020},\n",
            "year={2020}\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjf_j6F8CTmI"
      },
      "source": [
        "We can observe the output of the penultimate dense layer is proposed as embeddings. \r\n",
        "We will use it to extract songs embeddings from our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nth3VhRL8Y7i"
      },
      "source": [
        "# 3. Embeddings extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udu0-38TMsRq"
      },
      "source": [
        "We will fix sample rate at 16 kHz as it is required for the input of MusiCNN model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1RbRKrO8cMk"
      },
      "source": [
        "MUSICNN_SR = 16000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0Vp6apPCko2"
      },
      "source": [
        "def extract_mean_embedding(filename):\r\n",
        "  \"\"\"\r\n",
        "  Extract mean-temporal embedding from audio contained in filename\r\n",
        "\r\n",
        "  Args:\r\n",
        "    filename (str): Name of the audio file\r\n",
        "\r\n",
        "  Return:\r\n",
        "    Mean embedding of the song\r\n",
        "  \"\"\"\r\n",
        "  \r\n",
        "  # Load audiofile with essentia monoloader to resample the audios to the necessary sample rate in MusiCNN model\r\n",
        "  audio = es.MonoLoader(filename=filename, sampleRate=MUSICNN_SR)()\r\n",
        "\r\n",
        "  # Extract the embedding\r\n",
        "  musicnn_emb = es.TensorflowPredictMusiCNN(graphFilename=MODEL_PB, output='model/dense/BiasAdd')(audio)\r\n",
        "\r\n",
        "  # Compute mean-embedding across the frames\r\n",
        "  mean_emb = np.mean(musicnn_emb, axis=0)\r\n",
        "  mean_emb = mean_emb[np.newaxis, :]  # Each song is a 1x200 row vector\r\n",
        "\r\n",
        "  return mean_emb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaZuVc45DVM5"
      },
      "source": [
        "# This step may last several minutes\r\n",
        "embeddings = np.zeros((len(tracks), 200))  # N songs x 200 embedding-dim\r\n",
        "\r\n",
        "for i, track in enumerate(tracks.values()):\r\n",
        "  embeddings[i, :] = extract_mean_embedding(track.audio_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIUhaaJNXTk8",
        "outputId": "19763880-3758-4842-e0da-67c69bc30a5e"
      },
      "source": [
        "embeddings.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjFO5KV38kbY"
      },
      "source": [
        "# 4. Labels processing\r\n",
        "\r\n",
        "Build a label encoder to transform categorical labels to numerical ones in a range [0-9]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYFKOIOm8lrC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92c3c6e0-1198-478e-80f1-907aca00464d"
      },
      "source": [
        "label_encoder = preprocessing.LabelEncoder()\r\n",
        "label_encoder.fit(list({x.genre for x in tracks.values()}))\r\n",
        "label_encoder.classes_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['blues', 'classical', 'country', 'disco', 'hip-hop', 'jazz',\n",
              "       'metal', 'pop', 'reggae', 'rock'], dtype='<U9')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfQXCA0nE3uS",
        "outputId": "7655ebfd-db6f-45c3-de8c-6f4d24d4b6e1"
      },
      "source": [
        "labels = []\r\n",
        "labels_decoded = []\r\n",
        "track_ids = []\r\n",
        "\r\n",
        "for i, (track_id, track) in enumerate(tracks.items()):\r\n",
        "  print(f\"{i+1}/{len(tracks)}\", end=\"\\r\")\r\n",
        "  labels.append(int(label_encoder.transform([track.genre])[0]))\r\n",
        "  labels_decoded.append(track.genre)\r\n",
        "  track_ids.append(track_id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtGLhNJL829m"
      },
      "source": [
        "# 5. Save data\r\n",
        "\r\n",
        "Save new data to npy files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBixUSHV85e3"
      },
      "source": [
        "%mkdir emb_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ5sJ-0fFoOk"
      },
      "source": [
        "with open('emb_dataset/embeddings.npy', 'wb') as f:\r\n",
        "    np.save(f, embeddings)\r\n",
        "with open('emb_dataset/labels.npy', 'wb') as f:\r\n",
        "    np.save(f, labels)\r\n",
        "with open('emb_dataset/labels_decoded.npy', 'wb') as f:\r\n",
        "    np.save(f, labels_decoded)\r\n",
        "with open('emb_dataset/track_ids.npy', 'wb') as f:\r\n",
        "    np.save(f, track_ids)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}